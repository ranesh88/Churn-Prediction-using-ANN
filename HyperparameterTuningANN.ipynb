{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNTL/f82oHxW27AaencsQy+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranesh88/Churn-Prediction-using-ANN/blob/main/HyperparameterTuningANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pickle"
      ],
      "metadata": {
        "id": "mjHl17N1U7Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "id": "-djo5svcU7E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/content/Churn_Modelling.csv')\n",
        "data = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
        "\n",
        "label_encoder_gender = LabelEncoder()\n",
        "data['Gender'] = label_encoder_gender.fit_transform(data['Gender'])\n",
        "\n",
        "onehot_encoder_geo = OneHotEncoder(handle_unknown='ignore')\n",
        "geo_encoded = onehot_encoder_geo.fit_transform(data[['Geography']]).toarray()\n",
        "geo_encoded_df = pd.DataFrame(geo_encoded, columns=onehot_encoder_geo.get_feature_names_out(['Geography']))\n",
        "\n",
        "data = pd.concat([data.drop('Geography', axis=1), geo_encoded_df], axis=1)\n",
        "\n",
        "X = data.drop('Exited', axis=1)\n",
        "y = data['Exited']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Save encoders and scaler for later use\n",
        "with open('label_encoder_gender.pkl', 'wb') as file:\n",
        "    pickle.dump(label_encoder_gender, file)\n",
        "\n",
        "with open('onehot_encoder_geo.pkl', 'wb') as file:\n",
        "    pickle.dump(onehot_encoder_geo, file)\n",
        "\n",
        "with open('scaler.pkl', 'wb') as file:\n",
        "    pickle.dump(scaler, file)"
      ],
      "metadata": {
        "id": "F73X0ZHFU7Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikeras scikit-learn tensorflow\n"
      ],
      "metadata": {
        "id": "WprFZVNQYirT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Define the model function\n",
        "def create_model(input_dim, optimizer='adam', activation='relu'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, activation=activation))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # For binary classification\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create a function to perform manual grid search\n",
        "def manual_grid_search(X_train, y_train, X_val, y_val, param_grid):\n",
        "    best_score = -np.inf\n",
        "    best_params = {}  # Initialize best_params outside the loop to avoid the NameError\n",
        "\n",
        "    # Loop through the parameter grid\n",
        "    for optimizer in param_grid['optimizer']:\n",
        "        for activation in param_grid['activation']:\n",
        "            for epochs in param_grid['epochs']:\n",
        "                for batch_size in param_grid['batch_size']:\n",
        "                    print(f\"Training model with optimizer={optimizer}, activation={activation}, epochs={epochs}, batch_size={batch_size}\")\n",
        "\n",
        "                    # Create and train the model\n",
        "                    model = KerasClassifier(model=create_model, optimizer=optimizer, activation=activation, epochs=epochs, batch_size=batch_size, input_dim=X_train.shape[1])\n",
        "                    model.fit(X_train, y_train)\n",
        "\n",
        "                    # Evaluate the model\n",
        "                    score = model.score(X_val, y_val)\n",
        "                    print(f\"Score: {score}\")\n",
        "\n",
        "                    # Check if this model has the best score so far\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_params = {'optimizer': optimizer, 'activation': activation, 'epochs': epochs, 'batch_size': batch_size}\n",
        "\n",
        "    print(\"\\nBest score: \", best_score)\n",
        "    print(\"Best parameters: \", best_params)\n",
        "    return best_score, best_params\n",
        "\n",
        "\n",
        "# Define the parameter grid for manual grid search\n",
        "param_grid = {\n",
        "    'optimizer': ['adam', 'sgd'],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'epochs': [10, 20],\n",
        "    'batch_size': [32, 64]\n",
        "}\n",
        "\n",
        "# Example dataset (replace these with your actual dataset)\n",
        "# X, y are your features and target variable respectively\n",
        "X = np.random.rand(100, 10)  # Example feature data (100 samples, 10 features)\n",
        "y = np.random.randint(2, size=100)  # Example binary target data (0 or 1)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Run the manual grid search\n",
        "best_score, best_params = manual_grid_search(X_train, y_train, X_val, y_val, param_grid)\n",
        "\n",
        "# Output the best found parameters and score\n",
        "print(\"Best model's parameters:\", best_params)\n",
        "print(\"Best model's score:\", best_score)\n"
      ],
      "metadata": {
        "id": "YMz7FICze6nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7H53SERXfRuZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}